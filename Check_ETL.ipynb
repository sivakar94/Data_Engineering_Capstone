{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Engineering Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### PROJECT SUMMARY: \n",
    "The objective of this project was to create an Elt pipeline for I94 immigration, global land temperatures and happiness and Human development index datsets to form an analytics database on immigration events. \n",
    "\n",
    "A use case for this analytics database is to find immigration patterns for the US immigration department.\n",
    "\n",
    "For example, they could try to find answears to questions such as,\n",
    "\n",
    "- Do people from countries with warmer or cold climate immigrate to the US in large numbers?\n",
    "\n",
    "- Do people come from developed countries?\n",
    "\n",
    "- Does Freedom and Human development imply in the number of people coming in to the us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import from_unixtime, to_timestamp\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import datetime as dt\n",
    "from datetime import timedelta, datetime\n",
    "date_format = \"%Y-%m-%d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('credentials.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data = \"s3://capstoneprojectsiva/\"\n",
    "output_data = \"s3://capstoneprojectsiva/\"\n",
    "\n",
    "immigration_file_name = 'i94_apr16_sub.sas7bdat'\n",
    "temperature_file_name = 'GlobalLandTemperaturesByCity.csv'\n",
    "happiness_development_file_name = 'happiness_and_development.csv'\n",
    "\n",
    "country_code_file = input_data + \"i94cit.csv\"\n",
    "visa_code_file = input_data + \"i94visa.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_spark_immigration_data(df):\n",
    "    \"\"\"Clean immigration dataframe\n",
    "    :param df: spark dataframe with monthly immigration data\n",
    "    :return: clean dataframe\n",
    "    \"\"\"\n",
    "    total_records_count = df.count()\n",
    "    print(f'Total records in dataframe: {total_records_count:,}')\n",
    "    \n",
    "    # EDA has shown these columns to exhibit over 90% missing values, and hence we drop them\n",
    "    drop_columns = [\"visapost\", \"occup\", \"entdepu\", \"insnum\",\"count\", \"entdepa\", \"entdepd\", \"matflag\", \"dtaddto\", \"biryear\", \"admnum\",\"fltno\", \"airline\"]\n",
    "\n",
    "    df = df.drop(*drop_columns)    \n",
    "    # drop rows where all elements are missing\n",
    "    df = df.dropna(how='all')\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in dataframe: 3,096,313\n"
     ]
    }
   ],
   "source": [
    "df=clean_spark_immigration_data(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|dtadfile|gender|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|    null|  null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|20130811|     M|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|20160401|     M|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_immigration_fact_table(spark, df):\n",
    "    \"\"\"\n",
    "     This Function Creates visa dimension table\n",
    "    \n",
    "    :param df: spark dataframe of the immigration data\n",
    "    :output: path to write visa dimension table\n",
    "    :return: Visa dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    @udf()\n",
    "    def date_diff(date1, date2):\n",
    "    #Calculates the difference in days between two dates\n",
    "    \n",
    "        if date2 is None:\n",
    "            return None\n",
    "        else:\n",
    "            a = datetime.strptime(date1, date_format)\n",
    "            b = datetime.strptime(date2, date_format)\n",
    "            delta = b - a\n",
    "            return delta.days\n",
    "\n",
    "    date_format = \"%Y-%m-%d\"\n",
    "    \n",
    "    #User defined functions using Spark udf wrapper function to convert SAS dates into string dates in the format YYYY-MM-DD\n",
    "    convert_sas_udf = udf(lambda x: x if x is None else (timedelta(days=x) + datetime(1960, 1, 1)).strftime(date_format))\n",
    "    \n",
    "    df = df.withColumn(\"Arrival_date\", convert_sas_udf(col(\"arrdate\"))) \n",
    "    df = df.withColumn(\"Departure_date\", convert_sas_udf(col(\"depdate\")))\n",
    "    df = df.withColumn('Stay_period', date_diff(col('Arrival_date'), col('Departure_date')))\n",
    "    df = df.drop(*drop_columns) \n",
    "    \n",
    "    df.show(3)\n",
    "    #df.write.parquet(output_data + \"Immigration_Fact\", mode=\"overwrite\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+--------+------------+--------------+-----------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|dtadfile|gender|visatype|Arrival_date|Departure_date|Stay_period|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+--------+------------+--------------+-----------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|    null|  null|      B2|  2016-04-29|          null|       null|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|20130811|     M|      F1|  2016-04-07|          null|       null|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|20160401|     M|      B2|  2016-04-01|    2016-08-25|        146|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+--------+------------+--------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact=create_immigration_fact_table(spark,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_temperature_dimension_table(immigration_df):\n",
    "    \"\"\"\n",
    "    This Function Creates visa dimension table\n",
    "    \n",
    "    :param df: spark dataframe of the immigration data\n",
    "    :output: path to write visa dimension table\n",
    "    :return: Visa dataframe\n",
    "    \"\"\"\n",
    "    temp_df= spark.read.csv(\"../../data2/GlobalLandTemperaturesByCity.csv\",header=True, inferSchema=True)\n",
    "    temp_df = temp_df.groupby([\"Country\"])\\\n",
    "                            .agg({\"AverageTemperature\": \"avg\", \"Latitude\": \"first\", \"Longitude\": \"first\"})\\\n",
    "                            .withColumnRenamed('avg(AverageTemperature)', 'Temperature')\\\n",
    "                            .withColumnRenamed('first(Latitude)', 'Latitude')\\\n",
    "                            .withColumnRenamed('first(Longitude)', 'Longitude')\n",
    "        \n",
    "    temp_df = temp_df.withColumn('lower_country', lower(temp_df.Country))\n",
    "    temp_df = temp_df.drop(\"Country\")\n",
    "    temp_df.createOrReplaceTempView(\"temp_view\")\n",
    "\n",
    "    country_code = spark.read.csv(\"Lookup/i94cit.csv\", header=True, inferSchema=True)\n",
    "    country_code.createOrReplaceTempView(\"countrycode_view\")\n",
    "    \n",
    "    # create country dimension using SQL\n",
    "    Final_df=spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "        c.code as Country_Code,\n",
    "        t.lower_country as Country,\n",
    "        t.Temperature,\n",
    "        t.Latitude,\n",
    "        t.Longitude\n",
    "        from countrycode_view as c\n",
    "        LEFT JOIN temp_view as t\n",
    "        on c.country=t.lower_country\n",
    "        \"\"\"\n",
    "    ).distinct()\n",
    "    \n",
    "    Final_df = Final_df.dropna(subset=['Temperature'])\n",
    "    Final_df.show(3)\n",
    "    \n",
    "\n",
    "    return Final_df\n",
    "    \n",
    "    # write dimension to parquet file\n",
    "    #temp_df.write.parquet(output_data + \"visatype\", mode=\"overwrite\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+--------+---------+\n",
      "|Country_Code|     Country|       Temperature|Latitude|Longitude|\n",
      "+------------+------------+------------------+--------+---------+\n",
      "|         348|sierra leone|25.641010910058544|   8.84N|   13.78W|\n",
      "|         576| el salvador| 25.26285255093977|  13.66N|   90.00W|\n",
      "|         163|  uzbekistan|11.946573813309117|  40.99N|   72.43E|\n",
      "+------------+------------+------------------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df=create_temperature_dimension_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+--------+------------+--------------+-----------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|dtadfile|gender|visatype|Arrival_date|Departure_date|Stay_period|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+--------+------------+--------------+-----------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|    null|  null|      B2|  2016-04-29|          null|       null|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|20130811|     M|      F1|  2016-04-07|          null|       null|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|20160401|     M|      B2|  2016-04-01|    2016-08-25|        146|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+--------+------------+--------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_df=create_immigration_fact_table(spark, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_visa_dimension_table(immigration_df):\n",
    "    \"\"\"\n",
    "    This Function Creates visa dimension table\n",
    "    \n",
    "    :param df: spark dataframe of the immigration data\n",
    "    :output: path to write visa dimension table\n",
    "    :return: Visa dataframe\n",
    "    \"\"\"\n",
    "    visa_code_file=\"Lookup/i94visa.csv\"\n",
    "    visa_code = spark.read.csv(visa_code_file, header=True, inferSchema=True)\n",
    "    # Create Visa code view\n",
    "    visa_code.createOrReplaceTempView(\"visa_code_view\")\n",
    "    # Create Immiogration view\n",
    "    immigration_df.createOrReplaceTempView(\"immigration_view\")\n",
    "    \n",
    "    # create country dimension using SQL\n",
    "    visa=spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "        v.Stay_Purpose,\n",
    "        i.visatype,\n",
    "        i.Stay_period\n",
    "        from \n",
    "        immigration_view as i\n",
    "        JOIN visa_code_view as v\n",
    "        on i.i94visa = v.code\n",
    "        \"\"\"\n",
    "    ).distinct()\n",
    "        \n",
    "    visa_df = visa.withColumn('visa_key', monotonically_increasing_id())\n",
    "    visa_df.show(5)\n",
    "    \n",
    "    # write dimension to parquet file\n",
    "    #visa_df.write.parquet(output_data + \"visatype\", mode=\"overwrite\")\n",
    "\n",
    "    return visa_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----------+--------+\n",
      "|Stay_Purpose|visatype|Stay_period|visa_key|\n",
      "+------------+--------+-----------+--------+\n",
      "|     Student|      F1|         70|       0|\n",
      "|    Business|      E2|         23|       1|\n",
      "|    Business|      B1|         65|       2|\n",
      "|    Business|      E1|         67|       3|\n",
      "|    Pleasure|      B2|        113|       4|\n",
      "+------------+--------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visa_df=create_visa_dimension_table(fact_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_immigration_calendar_dimension(immigration_df):\n",
    "     \"\"\"\n",
    "     This Function Creates visa dimension table\n",
    "    \n",
    "    :param df: spark dataframe of the immigration data\n",
    "    :output: path to write visa dimension table\n",
    "    :return: Visa dataframe\n",
    "    \"\"\"\n",
    "            \n",
    "        \n",
    "     calendar_df=df.select([\"Arrival_date\",\"Departure_date\",\"Stay_period\"]).distinct()    \n",
    "\n",
    "     # expand df by adding other calendar columns\n",
    "     calendar_df = calendar_df.withColumn('arrival_day', dayofmonth('Arrival_date'))\n",
    "     calendar_df = calendar_df.withColumn('arrival_week', weekofyear('Arrival_date'))\n",
    "     calendar_df = calendar_df.withColumn('arrival_month', month('Arrival_date'))\n",
    "     calendar_df = calendar_df.withColumn('arrival_year', year('Arrival_date'))\n",
    "     calendar_df.show(4)\n",
    "        \n",
    "     # create an id field \n",
    "     calendar_df = calendar_df.withColumn('id_key', monotonically_increasing_id())\n",
    "\n",
    "     # write dimension to parquet file\n",
    "     #calendar_df.write.parquet(output_data + \"visatype\", mode=\"overwrite\")\n",
    "\n",
    "     return calendar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------+-------------+------------+---------------+-----------+\n",
      "|   arrdate|arrival_day|arrival_week|arrival_month|arrival_year|arrival_weekday|     id_key|\n",
      "+----------+-----------+------------+-------------+------------+---------------+-----------+\n",
      "|2016-04-22|         22|          16|            4|        2016|              6| 8589934592|\n",
      "|2016-04-15|         15|          15|            4|        2016|              6|25769803776|\n",
      "|2016-04-18|         18|          16|            4|        2016|              2|42949672960|\n",
      "|2016-04-09|          9|          14|            4|        2016|              7|68719476736|\n",
      "+----------+-----------+------------+-------------+------------+---------------+-----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calendar_df=create_immigration_calendar_dimension(fact_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_country_dimension_table(spark, temperature_df):\n",
    "    \n",
    "    country_code = spark.read.csv(\"Lookup/i94cit.csv\", header=True, inferSchema=True)\n",
    "    combined = country_code.join(temperature_df, country_code.country == temperature_df.lower_country, how=\"right\")\n",
    "    combined.createOrReplaceTempView(\"countrycode_temp_view\")\n",
    "\n",
    "    happiness_development = spark.read.csv(\"Lookup/happiness_and_development.csv\", header=True, inferSchema=True)\n",
    "    happiness_development.createOrReplaceTempView(\"H_D\")\n",
    "    \n",
    "    # create country dimension using SQL\n",
    "    Final_df=spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "        t.code Country_Code,\n",
    "        h.*\n",
    "        from countrycode_temp_view as t\n",
    "        LEFT JOIN H_D as h\n",
    "        on t.lower_country=h.country\n",
    "        \"\"\"\n",
    "    ).distinct()\n",
    "    \n",
    "    Final_df.show(3)\n",
    "    # write the dimension to a parquet file\n",
    "    #Final_country_df.write.parquet(output_data + \"country\", mode=\"overwrite\")\n",
    "\n",
    "    return Final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+------------------+--------+---------+---------+--------------+-----------------------+----------------------------+-------------------------+---------------------+\n",
      "|Country_Code|  Country|       Temperature|Latitude|Longitude|  Country|Social support|Healthy life expectancy|Freedom to make life choices|Perceptions of corruption|HumanDevelopmentIndex|\n",
      "+------------+---------+------------------+--------+---------+---------+--------------+-----------------------+----------------------------+-------------------------+---------------------+\n",
      "|         689|   brazil|21.902762979774167|  12.05S|   37.81W|   brazil|         0.882|                 66.601|                       0.804|                    0.756|                0.759|\n",
      "|         438|australia| 16.70146214247643|  34.56S|  138.16E|australia|          0.94|                   73.9|                       0.914|                    0.442|                0.939|\n",
      "|         369| ethiopia| 20.61152529673101|   8.84N|   38.11E| ethiopia|         0.764|                   59.0|                       0.752|                    0.761|                0.463|\n",
      "+------------+---------+------------------+--------+---------+---------+--------------+-----------------------+----------------------------+-------------------------+---------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Final_df=create_country_dimension_table(spark, temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+------------------+--------+---------+------------+--------------+-----------------------+----------------------------+-------------------------+---------------------+\n",
      "|Country_Code|             Country|       Temperature|Latitude|Longitude|     Country|Social support|Healthy life expectancy|Freedom to make life choices|Perceptions of corruption|HumanDevelopmentIndex|\n",
      "+------------+--------------------+------------------+--------+---------+------------+--------------+-----------------------+----------------------------+-------------------------+---------------------+\n",
      "|         689|              brazil|21.902762979774167|  12.05S|   37.81W|      brazil|         0.882|                 66.601|                       0.804|                    0.756|                0.759|\n",
      "|         438|           australia| 16.70146214247643|  34.56S|  138.16E|   australia|          0.94|                   73.9|                       0.914|                    0.442|                0.939|\n",
      "|         369|            ethiopia| 20.61152529673101|   8.84N|   38.11E|    ethiopia|         0.764|                   59.0|                       0.752|                    0.761|                0.463|\n",
      "|         584|                cuba|25.596881670099233|  20.09N|   76.78W|        cuba|         0.812|                   67.3|                       0.857|                    0.809|                0.777|\n",
      "|        null|congo (democratic...|23.240345757415405|   4.02S|   16.88E|        null|          null|                   null|                        null|                     null|                 null|\n",
      "|         255|             lebanon|18.759460757156063|  34.56N|   35.03E|     lebanon|         0.848|                 67.355|                       0.525|                    0.898|                0.757|\n",
      "|         151|             armenia| 8.375597043951773|  40.99N|   44.73E|     armenia|         0.799|                 67.055|                       0.825|                    0.629|                0.755|\n",
      "|         112|             germany| 8.482790790263826|  50.63N|    6.34E|     germany|         0.903|                   72.5|                       0.875|                     0.46|                0.936|\n",
      "|         386|               benin|26.975880208333386|   7.23N|    2.43E|       benin|         0.489|                 54.713|                       0.757|                    0.661|                0.515|\n",
      "|         339|               ghana|26.319964426877434|   5.63N|    0.00W|       ghana|         0.727|                 57.586|                       0.807|                    0.848|                0.592|\n",
      "|         213|               india|25.429224037736443|  29.74N|   73.85E|       india|         0.603|                 60.633|                       0.893|                    0.774|                 0.64|\n",
      "|         261|        saudi arabia| 24.86350878110749|  18.48N|   42.25E|saudi arabia|         0.891|                 66.603|                       0.877|                    0.684|                0.853|\n",
      "|         372|             eritrea|24.001515877771144|  15.27N|   39.17E|     eritrea|         0.781|                 56.101|                       0.709|                    0.855|                0.516|\n",
      "|         324|              angola|21.759715773235612|  12.05S|   13.15E|        null|          null|                   null|                        null|                     null|                 null|\n",
      "|         152|          azerbaijan| 11.11366381418095|  40.99N|   48.99E|  azerbaijan|         0.836|                 65.656|                       0.814|                    0.506|                0.757|\n",
      "|         155|          kazakhstan| 4.340299609693085|  50.63N|   82.39E|  kazakhstan|         0.952|                   65.2|                       0.853|                    0.733|                  0.8|\n",
      "|         696|           venezuela|  25.4824224495668|   8.84N|   68.92W|   venezuela|         0.861|                   66.7|                       0.615|                    0.827|                0.761|\n",
      "|         687|           argentina|16.999215885618334|  39.38S|   62.43W|   argentina|         0.898|                   69.0|                       0.828|                    0.834|                0.825|\n",
      "|         512|             bahamas| 24.78697831775695|  24.92N|   78.03W|     bahamas|         0.812|                   67.3|                       0.857|                    0.809|                0.807|\n",
      "|         735|          montenegro| 10.22104011370813|  42.59N|   19.64E|  montenegro|         0.858|                 68.699|                       0.708|                    0.812|                0.814|\n",
      "+------------+--------------------+------------------+--------+---------+------------+--------------+-----------------------+----------------------------+-------------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Final_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
